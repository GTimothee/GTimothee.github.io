<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> graphRAG implementation in Llama Index | Timothée Guédon </title> <meta name="author" content="Timothée Guédon"> <meta name="description" content="A graphRAG guide with llama index"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website, timothee, guedon, ai, deep-learning, data-science, generative-ai"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%9A%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://gtimothee.github.io/blog/2025/llamaindex-graphrag-implementation/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Timothée</span> Guédon </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">graphRAG implementation in Llama Index</h1> <p class="post-meta"> Created in March 17, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rag"> <i class="fa-solid fa-hashtag fa-sm"></i> rag</a>   ·   <a href="/blog/category/rag"> <i class="fa-solid fa-tag fa-sm"></i> rag</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="1-loading-the-data">1. Loading the data</h2> <ul> <li>Input: files or data in memory</li> <li>Methods: <ul> <li>Loading from files ? use SimpleDirectoryReader or FlatFileReader to load the files</li> <li>Loading from memory (.e.g. loading from a dataframe) ? convert data to Documents (<code class="language-plaintext highlighter-rouge">docs = [Document(text=sample['text']) for sample in docs]</code>)</li> </ul> </li> <li>Output: “Nodes” representing documents</li> <li>Ref: https://docs.llamaindex.ai/en/v0.10.19/module_guides/loading/node_parsers/modules.html</li> </ul> <h2 id="2-initial-preprocessing">2. Initial preprocessing</h2> <h3 id="transformations">Transformations</h3> <ul> <li>Input: Nodes</li> <li>Transformations can be used to perform preprocessing like chunking or metadata extraction for example.</li> <li>Output: Nodes</li> <li>Ref: https://docs.llamaindex.ai/en/stable/module_guides/loading/ingestion_pipeline/transformations/</li> </ul> <h3 id="chunking">Chunking</h3> <p>A type of transformation. Utilities for splitting are called splitters.</p> <ul> <li>Loading from files ? use SimpleFileNodeParser on the docs you retrieved with FlatFileReader. It will use the best splitter regarding the extension of the file you loaded.</li> <li>Loading from memory ? choose the best splitter depending on your data <ul> <li>simple text: TokenTextSplitter, SentenceSplitter (split on sentences, with overlap), SentenceWindowNodeParser (split on sentences, with sentence-level overlap put in metadata), MarkdownNodeParser, SemanticSplitterNodeParser (by Greg Kamradt, requires an embedding model)</li> <li>special format: JSONNodeParser, HTMLNodeParser, CodeSplitter</li> <li>meh: HierarchicalNodeParser (advanced, will see later)</li> </ul> </li> </ul> <p>ref: https://docs.llamaindex.ai/en/v0.10.19/module_guides/loading/node_parsers/modules.html</p> <h3 id="metadata-extraction">Metadata extraction</h3> <p>Adds metadata to the chunks.</p> <ul> <li>Allows: <ul> <li>better context for the llm to generate an answer (““chunk dreaming” - each individual chunk can have more “holistic” details, leading to higher answer quality given retrieved results.”)</li> <li>using metadata to improve the search (“disambiguate similar-looking passages.”)</li> </ul> </li> <li>Example extractors: <ul> <li>SummaryExtractor: extracts summaries, not only within the current text, but also within adjacent texts.</li> <li>QuestionsAnsweredExtractor: creates hypothetical question embeddings relevant to the document chunk</li> <li>TitleExtractor: in case the file name is not perfect, which happens a lot, you may want to extrapolate a file name representative of the content</li> <li>KeywordExtractor: “extracts entities (i.e. names of places, people, things) mentioned in the content of each Node” <ul> <li>default extraction model : https://huggingface.co/tomaarsen/span-marker-mbert-base-multinerd</li> <li>will use these keywords during the search</li> <li>See example here: https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/EntityExtractionClimate/</li> </ul> </li> <li>BaseExtractor: to build your own</li> </ul> </li> <li>third-party extractors: allows to define what you are looking for as an object. You can therefore reproduce all the above extractors and more! <ul> <li>MarvinMetadataExtractor</li> <li>PydanticProgramExtractor</li> </ul> </li> <li>Ref: https://docs.llamaindex.ai/en/stable/module_guides/indexing/metadata_extraction/</li> <li>Ref: https://docs.llamaindex.ai/en/stable/examples/metadata_extraction/…</li> </ul> <h3 id="example-implementation">Example implementation</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">llama_index.core.extractors</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">SummaryExtractor</span><span class="p">,</span>
    <span class="n">TitleExtractor</span><span class="p">,</span>
    <span class="n">KeywordExtractor</span><span class="p">,</span>
    <span class="n">QuestionsAnsweredExtractor</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="n">llama_index.extractors.entity</span> <span class="kn">import</span> <span class="n">EntityExtractor</span>
<span class="kn">from</span> <span class="n">llama_index.core.node_parser</span> <span class="kn">import</span> <span class="n">TokenTextSplitter</span>
<span class="kn">from</span> <span class="n">llama_index.core</span> <span class="kn">import</span> <span class="n">SimpleDirectoryReader</span>
<span class="kn">from</span> <span class="n">llama_index.core.ingestion</span> <span class="kn">import</span> <span class="n">IngestionPipeline</span>

<span class="c1"># chunking
</span><span class="n">text_splitter</span> <span class="o">=</span> <span class="nc">TokenTextSplitter</span><span class="p">(</span>
    <span class="n">separator</span><span class="o">=</span><span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">chunk_overlap</span><span class="o">=</span><span class="mi">128</span>
<span class="p">)</span>

<span class="c1"># metadata extraction
</span><span class="n">extractors</span> <span class="o">=</span> <span class="p">[</span>
    <span class="nc">TitleExtractor</span><span class="p">(</span><span class="n">nodes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">),</span>
    <span class="nc">QuestionsAnsweredExtractor</span><span class="p">(</span><span class="n">questions</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">),</span>
    <span class="nc">EntityExtractor</span><span class="p">(</span><span class="n">prediction_threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">),</span>
    <span class="nc">SummaryExtractor</span><span class="p">(</span><span class="n">summaries</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">prev</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">self</span><span class="sh">"</span><span class="p">],</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">),</span>
    <span class="nc">KeywordExtractor</span><span class="p">(</span><span class="n">keywords</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">llm</span><span class="o">=</span><span class="n">llm</span><span class="p">)</span>
<span class="p">]</span>

<span class="c1"># concat transformations
</span><span class="n">transformations</span> <span class="o">=</span> <span class="p">[</span><span class="n">text_splitter</span><span class="p">]</span> <span class="o">+</span> <span class="n">extractors</span>

<span class="n">uber_docs</span> <span class="o">=</span> <span class="nc">SimpleDirectoryReader</span><span class="p">(</span><span class="n">input_files</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">data/10k-132.pdf</span><span class="sh">"</span><span class="p">]).</span><span class="nf">load_data</span><span class="p">()</span>
<span class="n">pipeline</span> <span class="o">=</span> <span class="nc">IngestionPipeline</span><span class="p">(</span><span class="n">transformations</span><span class="o">=</span><span class="n">transformations</span><span class="p">,</span> <span class="n">in_place</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">show_progress</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">uber_nodes</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="n">documents</span><span class="o">=</span><span class="n">uber_docs</span><span class="p">)</span>
<span class="n">uber_nodes</span><span class="p">[</span><span class="mi">1</span><span class="p">].</span><span class="n">metadata</span>
</code></pre></div></div> <p>Example output: //TODO</p> <p>Note: You can also define the transformations globally using the ‘Settings’ object.</p> <h2 id="3-building-an-index">3. Building an index</h2> <p>Definition of an index from the docs:</p> <ul> <li>“With your data loaded, you now have a list of Document objects (or a list of Nodes). It’s time to build an Index over these objects so you can start querying them.”</li> <li>“In LlamaIndex terms, an Index is a data structure composed of Document objects, designed to enable querying by an LLM. Your Index is designed to be complementary to your querying strategy.”</li> <li>The index include a “store” which is where the data will be saved. By default it is in memory but you can also store on disk or in a database.</li> <li>We then query the index.</li> </ul> <p>Indexes:</p> <ul> <li>VectorStoreIndex: allows for vector search, computing embeddings for each chunk</li> <li>KnowledgeGraphIndex</li> <li>PropertyGraphIndex <ul> <li>collection of labelled nodes linked together by relationships</li> <li>labelled nodes have properties</li> <li>to build property graph you define a list of kg_extractors that will be applied on each chunk</li> <li>extracted data are added as metadata to the input chunk (called “llama-index” nodes, they are just the input text chunks you feed)</li> </ul> </li> </ul> <p>Example with propertygraphindex:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">index</span> <span class="o">=</span> <span class="n">PropertyGraphIndex</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span>
    <span class="n">nodes</span><span class="p">,</span>
    <span class="n">kg_extractors</span><span class="o">=</span><span class="p">[</span><span class="n">extractor1</span><span class="p">,</span> <span class="n">extractor2</span><span class="p">,</span> <span class="p">...],</span>
<span class="p">)</span>
</code></pre></div></div> <p>Example with knowledgegraphindex:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">llama_index.core</span> <span class="kn">import</span> <span class="n">KnowledgeGraphIndex</span>

<span class="n">index</span> <span class="o">=</span> <span class="n">KnowledgeGraphIndex</span><span class="p">.</span><span class="nf">from_documents</span><span class="p">(</span>
    <span class="n">nodes</span><span class="p">,</span>
    <span class="n">max_triplets_per_chunk</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">.from_documents</code> inner workings:</p> <ol> <li>Convert your docs to chunks (=Nodes) using “transformations” (passed as argument or defaults to Settings.transformations)</li> <li>create the instance form the nodes (means you could have parse your documents yourself and create the instance yourself with the constructor) <ul> <li>default is to embed_kg_nodes (True)</li> <li>show_progress for progress bar</li> <li>inside constructor, main method called is: <ul> <li>self._insert_nodes(nodes or [])</li> </ul> </li> </ul> <ul> <li>applies kg extractors on nodes</li> <li>builds two lists : one with all nodes and one with all relationships</li> <li>filters pure node duplciates between our list of all nodes and the list of nodes already in the store (filter applied on “node.id”)</li> <li>filter out duplicate llama nodes</li> <li>if _embed_kg_nodes: <ul> <li>embed nodes</li> <li>embed llama nodes</li> </ul> </li> <li>insert lalma nodes in property graph</li> <li>insert nodes in property graph</li> <li>insert relationships in property graphs</li> </ul> </li> </ol> <p>Save and load from disk:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># save
index.storage_context.persist("./storage")

# load
storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
</code></pre></div></div> <p>kg_extractors:</p> <ul> <li>SimpleLLMPathExtractor: extracts single-hop paths of the form (entity1, relation, entity2). Prompt can be customized. <ul> <li>warning: Tune the max_paths_per_chunk argument</li> </ul> </li> <li>ImplicitPathExtractor: not very useful in real case, it is when you already have the relationships in the data (so it reads the relationships rather than extracting them). No DL model is used.</li> <li>DynamicLLMPathExtractor: you can give a schema of node and relationship types that the llm will try to follow. In case no type fits its needs, it will add its own types.</li> <li>SchemaLLMPathExtractor: Follows a strict schema without possibility of creating new types.</li> </ul> <p>Stores:</p> <ul> <li>Default store: SimplePropertyGraphStore <ul> <li>EntityNode, Relation are nodes and relationships extracted from chunk</li> <li>EntityNode are linked to their source chunk modeled as TextNode using Relation with label HAS_SOURCE</li> </ul> </li> <li> <p>Supporting embeddings: Neo4jPropertyGraphStore, TiDBPropertyGraphStore, FalkorDBPropertyGraphStore</p> </li> <li>Ref: https://docs.llamaindex.ai/en/stable/understanding/indexing/indexing/</li> <li>Ref (property graph): https://docs.llamaindex.ai/en/stable/module_guides/indexing/lpg_index_guide/</li> </ul> <h2 id="4-querying-the-graph">4. Querying the graph</h2> <ul> <li>Build a query engine from the index, passing in the retrievers you want. <ul> <li>possible response modes: https://docs.llamaindex.ai/en/stable/module_guides/deploying/query_engine/response_modes/</li> <li>if you want to tune it, construct it yourself instead of using the as_query_engine method as it lacks parameters</li> </ul> </li> <li>Default retrievers: LLMSynonymRetriever and VectorContextRetriever (if embeddings are enabled). <ul> <li>To enable embeddings you must use a store that supports them. Default in memory store does not support embeddings.</li> </ul> </li> <li>Using the query engine: <ol> <li>it creates a query bundle from the query (see below)</li> <li>it calls the _query implementation of the query engine you chose. Basically this method just fetches nodes and pass them to the llm to generate a response.</li> </ol> </li> </ul> <p>query bundle:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Query bundle: 
  Can embed strings and images (give the image_path parameter)
  
  query_str (str): the original user-specified query string.
      This is currently used by all non embedding-based queries.
  custom_embedding_strs (list[str]): list of strings used for embedding the query.
      This is currently used by all embedding-based queries.
  embedding (list[float]): the stored embedding for the query.
</code></pre></div></div> <p>base retrievers:</p> <ul> <li>LLMSynonymRetriever: retrieve based on LLM generated keywords/synonyms <ul> <li>a prompt to extract synonyms</li> <li>fetch matching kg nodes (not llama nodes) and apply get_rel_map on them (gets triples up to a certain depth) =&gt; get nodes and neighborhood in form of triplets, then convert triplets to NodeWithScore</li> <li>I am wondering if by fetching the neighborhood they retrieve the text nodes (input chunks) as the same time, as I can see the text nodes being retrieved also, but I could not find somewhere else from which they could be retrieved.</li> </ul> </li> <li>VectorContextRetriever: retrieve based on embedded graph nodes</li> <li>CustomPGRetriever: easy to subclass and implement custom retrieval logic</li> </ul> <p>cypher-based retrievers</p> <ul> <li>TextToCypherRetriever: ask the LLM to generate cypher based on the schema of the property graph <ul> <li>NOTE: Since the SimplePropertyGraphStore is not actually a graph database, it does not support cypher queries.</li> </ul> </li> <li>CypherTemplateRetriever: use a cypher template with params inferred by the LLM <ul> <li>This is a more constrained version of the TextToCypherRetriever. Rather than letting the LLM have free-range of generating any cypher statement, we can instead provide a cypher template and have the LLM fill in the blanks.</li> </ul> </li> </ul> <p>Example of passing the retrievers list:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create a retriever
</span><span class="n">retriever</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">as_retriever</span><span class="p">(</span><span class="n">sub_retrievers</span><span class="o">=</span><span class="p">[</span><span class="n">retriever1</span><span class="p">,</span> <span class="n">retriever2</span><span class="p">,</span> <span class="p">...])</span>

<span class="c1"># create a query engine
</span><span class="n">query_engine</span> <span class="o">=</span> <span class="n">index</span><span class="p">.</span><span class="nf">as_query_engine</span><span class="p">(</span>
    <span class="n">sub_retrievers</span><span class="o">=</span><span class="p">[</span><span class="n">retriever1</span><span class="p">,</span> <span class="n">retriever2</span><span class="p">,</span> <span class="p">...]</span>
<span class="p">)</span>
</code></pre></div></div> <p>Simple example for creating a query engine from an index:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>query_engine = index.as_query_engine(
    include_text=True,
    response_mode="tree_summarize",
    embedding_mode="hybrid",
    similarity_top_k=5,
)
response = query_engine.query(
    "Tell me more about what the author worked on at Interleaf",
)
</code></pre></div></div> <p>interesting methods on graph stores:</p> <ul> <li>graph_store.get_rel_map([entity_node], depth=2): gets triples up to a certain depth</li> <li>graph_store.get_llama_nodes([‘id1’]): gets the original text nodes</li> <li>graph_store.structured_query(“<cypher query="">") - runs a cypher query (assuming the graph store supports it)</cypher> </li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/patterns-to-problems-thoughts/">Stop Learning Patterns, Start Solving Problems - Lessons from Biology and Engineering</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/embedding-model-fine-tuning/">Embedding model fine-tuning</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/homemade-keyword-retriever/">How to write your own keyword retriever in 5 minutes</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/simple-test-procedure-for-rag/">Test procedure and metrics for RAG in 5 minutes</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Timothée Guédon. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let theme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===theme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"My last projects",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"post-graphrag-implementation-in-llama-index",title:"graphRAG implementation in Llama Index",description:"A graphRAG guide with llama index",section:"Posts",handler:()=>{window.location.href="/blog/2025/llamaindex-graphrag-implementation/"}},{id:"post-stop-learning-patterns-start-solving-problems-lessons-from-biology-and-engineering",title:"Stop Learning Patterns, Start Solving Problems - Lessons from Biology and Engineering",description:"This blog explores the shift from rigid categorization to creative problem-solving, drawing parallels between biology&#39;s evolution and modern engineering practices.",section:"Posts",handler:()=>{window.location.href="/blog/2025/patterns-to-problems-thoughts/"}},{id:"post-embedding-model-fine-tuning",title:"Embedding model fine-tuning",description:"Embedding model fine-tuning",section:"Posts",handler:()=>{window.location.href="/blog/2024/embedding-model-fine-tuning/"}},{id:"post-how-to-write-your-own-keyword-retriever-in-5-minutes",title:"How to write your own keyword retriever in 5 minutes",description:"Tutorial on how to write your own keyword retriever using the whoosh library",section:"Posts",handler:()=>{window.location.href="/blog/2024/homemade-keyword-retriever/"}},{id:"post-test-procedure-and-metrics-for-rag-in-5-minutes",title:"Test procedure and metrics for RAG in 5 minutes",description:"Simple test procedure and metrics for your RAG in 5 minutes",section:"Posts",handler:()=>{window.location.href="/blog/2024/simple-test-procedure-for-rag/"}},{id:"news-rag-agent-with-neo4j-tuto-update",title:"RAG agent with Neo4j tuto update",description:"",section:"News",handler:()=>{window.location.href="/news/2025-02-11-RAG-agent-with-Neo4j-tuto-update/"}},{id:"news-new-repo-on-rag-experiments",title:"New repo on RAG experiments",description:"",section:"News",handler:()=>{window.location.href="/news/2024-10-27-RAG-experiments-repo/"}},{id:"news-hello-world",title:"Hello world",description:"",section:"News",handler:()=>{window.location.href="/news/2024-06-12-Hello-world!/"}},{id:"projects-smolagent-custom-tools",title:"Smolagent custom tools",description:"Tools I built and shared with the community while building an agent with the smolagents library.",section:"Projects",handler:()=>{window.location.href="/projects/1_smolagents_tools_project/"}},{id:"projects-knowledge-graphs-project",title:"Knowledge graphs project",description:"Experimenting with knowledge graphs and RAG applications",section:"Projects",handler:()=>{window.location.href="/projects/2_knowledge_graphs_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%74%69%6D%6F%74%68%65%65.%67%75%65%64%6F%6E@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>