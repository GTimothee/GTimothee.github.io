<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://gtimothee.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gtimothee.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-23T10:13:02+00:00</updated><id>https://gtimothee.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Stop Learning Patterns, Start Solving Problems - Lessons from Biology and Engineering</title><link href="https://gtimothee.github.io/blog/2025/patterns-to-problems-thoughts/" rel="alternate" type="text/html" title="Stop Learning Patterns, Start Solving Problems - Lessons from Biology and Engineering"/><published>2025-01-23T00:00:00+00:00</published><updated>2025-01-23T00:00:00+00:00</updated><id>https://gtimothee.github.io/blog/2025/patterns-to-problems-thoughts</id><content type="html" xml:base="https://gtimothee.github.io/blog/2025/patterns-to-problems-thoughts/"><![CDATA[<blockquote> <p>Throughout history, humanity’s approach to understanding and creating has shifted dramatically. From categorizing the natural world to designing complex systems, there has always been a tension between rigid frameworks and creative exploration. This shift is especially evident in two seemingly different domains: biology and engineering. Both fields have evolved from emphasizing predefined patterns to embracing problem-solving as a dynamic, imaginative process. This evolution holds valuable lessons for anyone navigating the complexities of modern innovation.</p> </blockquote> <h2 id="the-era-of-categorization-biologys-early-days">The Era of Categorization: Biology’s Early Days</h2> <p>In the 18th and 19th centuries, biology was driven by a need to classify and categorize. Scientists like Carl Linnaeus created taxonomies—hierarchical systems that sorted organisms into kingdoms, phyla, genera, and species. This approach made sense for its time. The world was vast and uncharted, and organizing it into neat boxes was the first step toward understanding it.</p> <p>While taxonomy was foundational, it had limitations. It treated nature as static and segmented rather than interconnected and dynamic. This mindset began to shift with Charles Darwin’s theory of evolution. Instead of viewing species as fixed entities, Darwin revealed them as evolving, adapting systems. This sparked a revolution: biology was no longer just about classification; it became about understanding processes, relationships, and systems.</p> <p>The shift continued with the rise of genetics, molecular biology, and systems biology. Researchers began focusing on how things worked, not just what they were. Patterns were still useful, but they became tools, not endpoints. This flexible, process-driven approach unlocked profound insights into the complexity of life.</p> <h2 id="the-parallel-in-engineering-learning-to-build">The Parallel in Engineering: Learning to Build</h2> <p>A similar story is unfolding in engineering education and practice. Many engineering courses today emphasize predefined patterns and methodologies. Whether it’s software design patterns, system architectures, or AI frameworks, students are often taught that there are “N distinct ways” to build a system and that learning these patterns is essential.</p> <p>While these patterns can provide helpful starting points, they risk becoming constraints. Rigidly adhering to predefined templates limits creativity and discourages problem-solving. Real-world problems rarely fit neatly into predefined categories, and innovation often requires breaking away from existing molds.</p> <p>From my experience working on AI agents, I’ve found that the most valuable insights come not from memorizing patterns but from confronting challenges directly. When my team and I started building AI systems, we didn’t begin with a library of patterns. Instead, we faced the problems head-on, experimented, and let solutions emerge organically. The patterns we discovered were born out of necessity and tailored to our specific use cases. They weren’t taught; they arose naturally.</p> <h2 id="why-problem-solving-beats-pattern-learning">Why Problem-Solving Beats Pattern Learning</h2> <p>The key takeaway from both biology and engineering is this: predefined patterns can inspire, but they shouldn’t constrain. Here’s why problem-solving is a more effective approach:</p> <ul> <li>Creativity Thrives Without Limits: When you’re not bound by established patterns, you’re free to explore unconventional solutions. This often leads to breakthroughs that wouldn’t arise within rigid frameworks.</li> <li>Patterns Are Context-Specific: What works in one scenario might not work in another. Discovering solutions tailored to your unique challenges ensures better outcomes.</li> <li>Learning by Doing: Problem-solving fosters deeper understanding. Instead of memorizing abstract patterns, you gain hands-on experience that stays with you.</li> <li>Flexibility in Innovation: When new challenges arise, a problem-solving mindset equips you to adapt, while a pattern-driven approach may leave you stuck searching for the “right” template.</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>For students in AI, I would recommend not spending too much time learning patterns or design recommendations. Instead, focus on solving problems and understanding the foundational principles and mathematics behind a subject. When you engage directly with challenges, the patterns will arise naturally.</p> <p>I’ve experienced this myself when learning about different types of recurrent neural networks (RNNs). Early on, I spent too much time trying to memorize the various types and their applications. But when I shifted my mindset and started thinking about real-world problems, the patterns became obvious. For example, in translation tasks, it’s clear that the architecture naturally forms an N-to-N schema. By focusing on applications and problem-solving, you’ll gain a deeper understanding and develop solutions that fit your unique use cases. So, let go of rigid frameworks, and let curiosity and creativity guide your learning journey.</p> <p>Medium link: https://medium.com/@timothee.guedon/stop-learning-patterns-start-solving-problems-lessons-from-biology-and-engineering-d663c35a834c</p>]]></content><author><name></name></author><category term="engineering-experience"/><category term="science"/><category term="engineering"/><category term="biology"/><category term="problem-solving"/><category term="innovation"/><category term="creativity"/><category term="patterns"/><category term="systems-thinking"/><category term="research"/><category term="AI"/><category term="learning"/><category term="exploration"/><summary type="html"><![CDATA[This blog explores the shift from rigid categorization to creative problem-solving, drawing parallels between biology's evolution and modern engineering practices.]]></summary></entry><entry><title type="html">Embedding model fine-tuning</title><link href="https://gtimothee.github.io/blog/2024/embedding-model-fine-tuning/" rel="alternate" type="text/html" title="Embedding model fine-tuning"/><published>2024-11-24T00:00:00+00:00</published><updated>2024-11-24T00:00:00+00:00</updated><id>https://gtimothee.github.io/blog/2024/embedding-model-fine-tuning</id><content type="html" xml:base="https://gtimothee.github.io/blog/2024/embedding-model-fine-tuning/"><![CDATA[<blockquote> <p>In this blog post I show you how I fine-tune an embedding model for RAG.</p> </blockquote> <h2 id="motivation">Motivation</h2> <p>This post is an updated version of an <a href="https://www.philschmid.de/fine-tune-embedding-model-for-rag">old post</a> from Philip Schmidt. Indeed, I recently used his post as a basis to train my own embedding model for a RAG system. However, I faced two major challenges:</p> <ul> <li>allowing PEFT and LoRa so that the training fits on a smaller GPU</li> <li>adding my custom tokens so that the model understand technical documentation</li> </ul> <p>This blog post will follow these steps:</p> <ol> <li>try to reproduce the initial blog post</li> <li>add peft and LoRa</li> <li>add caching</li> <li>add custom tokens</li> </ol> <p>At each step we will be evaluating the results and comparing them to the initial results.</p> <h2 id="reproducing-the-initial-blog-post">Reproducing the initial blog post</h2> <p>// in progress</p> <p>tf32=True, # use tf32 precision bf16=True, # use bf16 precision</p> <p>creates error</p> <p>ValueError: –tf32 requires Ampere or a newer GPU arch, cuda&gt;=11 and torch&gt;=1.7</p> <p>if you encounter ‘NameError: name ‘IterableDataset’ is not defined’, just update sentence-transformers</p> <p>“MultipleNegativesRankingLoss will always consider the first column as the anchor and the second as the positive, regardless of the dataset column names.” Sp we need to do: “train_dataset=train_dataset.select_columns( [“anchor”, “positive”] )”</p> <h2 id="references">References</h2> <ul> <li><a href="https://www.philschmid.de/fine-tune-embedding-model-for-rag">Philip Schmidt’s post</a></li> </ul>]]></content><author><name></name></author><category term="rag"/><category term="embedding"/><category term="model"/><category term="training"/><category term="fine-tuning"/><category term="rag"/><summary type="html"><![CDATA[Embedding model fine-tuning]]></summary></entry><entry><title type="html">How to write your own keyword retriever in 5 minutes</title><link href="https://gtimothee.github.io/blog/2024/homemade-keyword-retriever/" rel="alternate" type="text/html" title="How to write your own keyword retriever in 5 minutes"/><published>2024-10-24T00:00:00+00:00</published><updated>2024-10-24T00:00:00+00:00</updated><id>https://gtimothee.github.io/blog/2024/homemade-keyword-retriever</id><content type="html" xml:base="https://gtimothee.github.io/blog/2024/homemade-keyword-retriever/"><![CDATA[<blockquote> <p>In this blog post I give you a tutorial on how to build a simple yet efficient keyword retriever for a RAG system, using the Whoosh library.</p> </blockquote> <h2 id="motivation">Motivation</h2> <p>I found myself building a RAG system for question answering, and it is not as easy as you might think.</p> <p>The main problem comes when dealing with technical documentation. Implementing a similarity search-based RAG will not help you much. Firstly, the embedding model does not know the keywords, so it cannot embed them properly. Secondly, when a user asks a question, the text is rarely “similar” to the chunk that contains the answer, if that makes sense.</p> <p>Although it does not solve all your problems, a first step towards improving your RAG in this setting is well known: you shall add a keyword retriever in addition to the similarity search-based retriever, as the both of them are complementary.</p> <p>This post will be useful for you in at least two ways: if you are curious about how a keyword retriever works in practice, and if, like me, you are using a database that does not have a keyword retriever included. Some might argue that I could have use the retriever from langchain but from what I understood it either loads all the documents in memory (not optimal) or needs elasticsearch as a backend, which my team and I did not want to do, as we weren’t using it for anything else in the project at hand.</p> <h2 id="whoosh">Whoosh</h2> <blockquote> <p>Before diving into the practical stuff, let me introduce Whoosh, which I will use to build the keyword retriever.</p> </blockquote> <p><a href="https://github.com/mchaput/whoosh">Whoosh</a> is a popular, fast, pure Python search engine library designed for adding search functionality to applications. It’s often used for indexing and searching textual data within Python applications, and it’s popular in cases where a lightweight, easy-to-integrate search solution is required.</p> <p>Key Features of Whoosh:</p> <ul> <li>Written in Pure Python: Whoosh is implemented entirely in Python, which makes it easy to install and run without external dependencies or a specific backend.</li> <li>Full-Text Search: It provides full-text search capabilities, including indexing and retrieving text-based data efficiently.</li> <li>Customizable: Whoosh is highly flexible and allows developers to customize search functionality with features like tokenizers, filters, and analyzers.</li> <li>Simple Integration: Whoosh is lightweight and can be embedded directly into Python applications without the need for external services, making it ideal for smaller-scale applications.</li> <li>Phrase and Boolean Search: Supports phrase searches, Boolean operators (AND, OR, NOT), and other advanced search features.</li> </ul> <p>Important note about Whoosh: It is now unmaintained, so you may want to use <a href="https://github.com/Sygil-Dev/whoosh-reloaded">“Whoosh Reloaded”</a> instead, a fork and continuation of the Whoosh project, which is actively maintained. The code I will give here works for Whoosh as well as for Whoosh Reloaded.</p> <h2 id="building-the-index">Building the index</h2> <p>The first thing we need to do is to build a reverse index. Whoosh gives a clear definition in its documentation:</p> <blockquote> <p>A reverse index is “Basically a table listing every word in the corpus, and for each word, the list of documents in which it appears. It can be more complicated (the index can also list how many times the word appears in each document, the positions at which it appears, etc.) but that’s how it basically works.” (Whoosh documentation)</p> </blockquote> <p>To build a second retriever for my RAG system, I reuse the chunks from the Chroma database I built for a previous blog post. That way, I will have two complementary retievers, retrieving on the same chunks. I build a little generator as follows:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">database_iterator</span><span class="p">(</span><span class="n">database_dirpath</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
    <span class="n">client</span> <span class="o">=</span> <span class="nf">get_db</span><span class="p">(</span><span class="n">database_dirpath</span><span class="p">)</span>
    <span class="n">offset</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">while</span> <span class="n">offset</span> <span class="o">&lt;</span> <span class="n">client</span><span class="p">.</span><span class="n">_collection</span><span class="p">.</span><span class="nf">count</span><span class="p">():</span>
        <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">offset=</span><span class="si">{</span><span class="n">offset</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">client</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">offset</span><span class="o">=</span><span class="n">offset</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
        <span class="n">offset</span> <span class="o">+=</span> <span class="nf">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">])</span>
        <span class="k">yield</span> <span class="n">batch</span>
</code></pre></div></div> <p>Find the full script <a href="https://github.com/GTimothee/RAG_experiments/blob/main/blogs/keyword_retriever/chromadb_iterator.py">here</a></p> <p>The next step consists in building a Schema.</p> <blockquote> <p>“Whoosh requires that you specify the fields of the index before you begin indexing. The Schema associates field names with metadata about the field, such as the format of the postings and whether the contents of the field are stored in the index.” (Whoosh documentation)</p> </blockquote> <p>The schema represents a document in the index. We will use <em>TEXT</em> and <em>STORED</em> fields, for each document. The <em>TEXT</em> field specifies the text to be indexed. The document and its terms will be added to the reverse index. The <em>STORED</em> fields are the fields that will be retrieved, if the document is retrieved.</p> <p>Here is my very simple schema:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">whoosh.fields</span> <span class="kn">import</span> <span class="n">Schema</span><span class="p">,</span> <span class="n">TEXT</span><span class="p">,</span> <span class="n">STORED</span>
<span class="kn">from</span> <span class="n">whoosh.analysis</span> <span class="kn">import</span> <span class="n">StemmingAnalyzer</span>

<span class="n">stem_ana</span> <span class="o">=</span> <span class="nc">StemmingAnalyzer</span><span class="p">()</span>
<span class="n">schema</span> <span class="o">=</span> <span class="nc">Schema</span><span class="p">(</span>
    <span class="n">content</span><span class="o">=</span><span class="nc">TEXT</span><span class="p">(</span><span class="n">analyzer</span><span class="o">=</span><span class="n">stem_ana</span><span class="p">),</span>
    <span class="n">text_content</span><span class="o">=</span><span class="n">STORED</span><span class="p">,</span>
    <span class="n">metadata</span><span class="o">=</span><span class="n">STORED</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>As you can see, I index the content of the document, and I store the content and the metadata. I also added a custom analyzer. The goal of the StemmingAnalyzer is to store the stems of the words, instead of the words themselves, which allows to generalize better.</p> <p>More about analyzers: “An analyzer is a function […] that takes a unicode string and returns a generator of tokens”. (Whoosh documentation)</p> <p>More about the StemmingAnalyzer: It is a “pre-packaged analyzer that combines a tokenizer, lower-case filter, optional stop filter, and stem filter” (Whoosh documentation)</p> <p>We can now create an index from our schema…</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">whoosh.index</span> <span class="kn">import</span> <span class="n">create_in</span>

<span class="n">index_dirpath</span> <span class="o">=</span> <span class="c1"># where you want to store your index
</span><span class="n">ix</span> <span class="o">=</span> <span class="nf">create_in</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="n">index_dirpath</span><span class="p">),</span> <span class="n">schema</span><span class="p">)</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">ix</span><span class="p">.</span><span class="nf">writer</span><span class="p">()</span>
</code></pre></div></div> <p>…And load our chunks into the index:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nf">database_iterator</span><span class="p">(</span><span class="sh">'</span><span class="s">data/chroma_db_1000</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">])</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">batch_size</span> 

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
        <span class="n">writer</span><span class="p">.</span><span class="nf">add_document</span><span class="p">(</span>
            <span class="n">text_content</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
            <span class="n">content</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">][</span><span class="n">i</span><span class="p">],</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">batch</span><span class="p">[</span><span class="sh">"</span><span class="s">metadatas</span><span class="sh">"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="p">)</span>

<span class="n">writer</span><span class="p">.</span><span class="nf">commit</span><span class="p">()</span>
</code></pre></div></div> <p>Find the full script <a href="https://github.com/GTimothee/RAG_experiments/blob/main/blogs/keyword_retriever/build_index.py">here</a></p> <h2 id="querying-the-index">Querying the index</h2> <p>Now that we have the index, let us write the retriever itself. Given a query, the retrieving process is pretty simple:</p> <ol> <li>preprocess the query: we want to apply the same preprocessing to the query that to the indexed documents, so that we can use the words stems for retrieval.</li> <li>translate the query into Whoosh query language (the same way we would translate our string into SQL for a SQL database)</li> <li>use that query against the index, leveraging a dedicated search algorithm</li> </ol> <p>To preprocess the query we remove stopwords using the <em>nltk</em> package, and we stem the words using the <code class="language-plaintext highlighter-rouge">whoosh.lang.porter.stem</code> function. Let us first download the stopwords (they will be downloaded only once).</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">nltk</span> 
<span class="kn">from</span> <span class="n">nltk.corpus</span> <span class="kn">import</span> <span class="n">stopwords</span>

<span class="n">stop_words</span> <span class="o">=</span> <span class="nf">set</span><span class="p">(</span><span class="n">stopwords</span><span class="p">.</span><span class="nf">words</span><span class="p">(</span><span class="sh">"</span><span class="s">english</span><span class="sh">"</span><span class="p">))</span>
<span class="n">stop_words</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="sh">"</span><span class="s">?</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p>For step 2, it is as simple as initializing a query parser. I use a simple QueryParser, but if you have several fields you want to look into at retrieval time, you can use the MultifieldParser instead. By default, the parser matches a document that contains all the terms specified in the query (using the AND operator). For example, “physically based rendering” will be parser “physically AND based AND rendering”. I change this behaviour by using the OR operator instead, and the “qparser.OrGroup.factory(0.9)” statement allows to give more weight to a document if it contains the words we are looking for multiple times.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">whoosh</span> <span class="kn">import</span> <span class="n">qparser</span>
<span class="kn">from</span> <span class="n">whoosh.qparser</span> <span class="kn">import</span> <span class="n">QueryParser</span>
<span class="kn">from</span> <span class="n">whoosh.index</span> <span class="kn">import</span> <span class="n">open_dir</span>

<span class="n">parser</span> <span class="o">=</span> <span class="nc">QueryParser</span><span class="p">(</span>
    <span class="sh">"</span><span class="s">content</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">ix</span><span class="p">.</span><span class="n">schema</span><span class="p">,</span>
    <span class="n">group</span><span class="o">=</span><span class="n">qparser</span><span class="p">.</span><span class="n">OrGroup</span><span class="p">.</span><span class="nf">factory</span><span class="p">(</span><span class="mf">0.9</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">ix</span> <span class="o">=</span> <span class="nf">open_dir</span><span class="p">(</span><span class="n">index_dirpath</span><span class="p">)</span>
</code></pre></div></div> <p>Putting everything together, I get this preprocessing function:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">nltk.tokenize</span> <span class="kn">import</span> <span class="n">word_tokenize</span>
<span class="kn">from</span> <span class="n">whoosh.lang.porter</span> <span class="kn">import</span> <span class="n">stem</span>

<span class="k">def</span> <span class="nf">_process_query</span><span class="p">(</span><span class="n">query</span><span class="p">):</span>
    <span class="c1"># tokenize 
</span>    <span class="n">word_tokens</span> <span class="o">=</span> <span class="nf">word_tokenize</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">tokenized query: </span><span class="si">{</span><span class="n">word_tokens</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="c1"># stem and filter out stop words
</span>    <span class="n">filtered_query</span> <span class="o">=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span><span class="p">.</span><span class="nf">join</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="nf">stem</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word_tokens</span>
            <span class="k">if</span> <span class="n">word</span><span class="p">.</span><span class="nf">lower</span><span class="p">()</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">stop_words</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">stemmed-filtered_query: </span><span class="si">{</span><span class="n">filtered_query</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

    <span class="n">parsed_query</span> <span class="o">=</span> <span class="n">parser</span><span class="p">.</span><span class="nf">parse</span><span class="p">(</span><span class="n">filtered_query</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">parsed_query: </span><span class="si">{</span><span class="n">parsed_query</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">parsed_query</span>
</code></pre></div></div> <p>We can now use the ix.searcher method to search for the most relevant documents:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">whoosh</span> <span class="kn">import</span> <span class="n">scoring</span>
<span class="kn">from</span> <span class="n">langchain_core.documents</span> <span class="kn">import</span> <span class="n">Document</span>

<span class="n">k</span> <span class="o">=</span> <span class="c1"># integer representing the number of documents you want to retrieve
</span><span class="n">query</span> <span class="o">=</span> <span class="c1"># a string
</span>
<span class="k">with</span> <span class="n">ix</span><span class="p">.</span><span class="nf">searcher</span><span class="p">(</span><span class="n">weighting</span><span class="o">=</span><span class="n">scoring</span><span class="p">.</span><span class="nc">BM25F</span><span class="p">())</span> <span class="k">as</span> <span class="n">searcher</span><span class="p">:</span>
    <span class="n">formatted_query</span> <span class="o">=</span> <span class="nf">_process_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">searcher</span><span class="p">.</span><span class="nf">search</span><span class="p">(</span><span class="n">formatted_query</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="nc">Document</span><span class="p">(</span>
            <span class="n">metadata</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">metadata</span><span class="sh">"</span><span class="p">],</span> 
            <span class="n">page_content</span><span class="o">=</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">text_content</span><span class="sh">"</span><span class="p">]</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">result</span> <span class="ow">in</span> <span class="n">results</span>
    <span class="p">]</span>
</code></pre></div></div> <p>As you can see, we retrieve the STORED fields “text_content” and “metadata” from the schema.</p> <p>Find the full script <a href="https://github.com/GTimothee/RAG_experiments/blob/main/library/keyword_retriever.py">here</a></p> <h2 id="the-search-algorithm">The search algorithm</h2> <p>As you may notice, I am using the BM25F algorithm (the default in Whoosh) to perform the search. From what I saw, only the ‘frequency-based’, ‘TF-IDF’ and ‘BM25F’ algorithms are supported in Whoosh. Whoosh-reloaded supports two additional algorithms: PL2 and DFREE. The BM25 algorithm is considered better than TF-IDF and is pretty robust. PL2 and DFREE can be better depending on the dataset at hand, but do not guarantee better results.</p> <p>The BM25F algorithm is a variant of the BM25 algorithm that enables searching over multiple fields at the same time, which suits Whoosh well as we can define a schema with multiple fields to look into.</p> <h2 id="evaluation">Evaluation</h2> <p>On the first 200 samples of the huggingface documentation dataset, the traditional RAG with similarity search gave: 73% mean completeness and 52% mean conciseness. Here is the associated disrtibution of the target document rank in the results:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.0    100
1.0     19
3.0     11
2.0     11
</code></pre></div></div> <p>It means that for half of the samples, we were able to find the target chunk with rank 0.</p> <p>I tried the same pipeline, replacing the similarity search-based retriever by the keyword retriever we built. I got 80% mean completeness and 56% mean conciseness. Here is the associated disrtibution of the target document rank in the results:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>0.0    132
1.0     19
3.0      8
2.0      4
</code></pre></div></div> <p>We can see that the keyword retiever actually performs better than the similarity search based retriever ! Depending on the dataset, it may not always be the case, but the point is that a keyword retriever actually performs pretty good. In a future blog post, we will combine both retrievers and add a reranker on top to get the best of both worlds and hopefully get better results than using only one of the retrievers.</p> <p>If you want the details and the code of the evaluation framework I used, checkout the dedicated <a href="https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag/">blog post</a>.</p> <h2 id="conclusion">Conclusion</h2> <p>We saw how to build a robust yet simple keyword retriever using Whoosh.</p> <p><a href="https://medium.com/@timothee.guedon/motivation-b242b05c1e78">Medium link</a></p>]]></content><author><name></name></author><category term="rag"/><category term="keyword-retriever"/><category term="rag"/><category term="whoosh"/><category term="python"/><summary type="html"><![CDATA[Tutorial on how to write your own keyword retriever using the whoosh library]]></summary></entry><entry><title type="html">Test procedure and metrics for RAG in 5 minutes</title><link href="https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag/" rel="alternate" type="text/html" title="Test procedure and metrics for RAG in 5 minutes"/><published>2024-10-04T00:00:00+00:00</published><updated>2024-10-04T00:00:00+00:00</updated><id>https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag</id><content type="html" xml:base="https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag/"><![CDATA[<blockquote> <p>In this blog post I show you how I build my simple evaluation procedure for a basic Q/A RAG system without use of external library or anything fancy. All you need is… an existing RAG system and to have access to an LLM. I also introduce two metrics I usually use to evaluate RAG systems, which are simple, quite robust, and easily interpretable in my opinion: completeness and conciseness.</p> </blockquote> <h2 id="motivation">Motivation</h2> <p><em>When you work in R&amp;D like me, you have limited time to build a PoC.</em> When working on RAG systems and AI agents in general, it is already a pain to select your tools, learn how to use them and build something robust. I will not explain you why you also need to evaluate the system, as it is pretty obvious.</p> <p><em>So you are faced with the following problem</em>: How do I focus on what I build, while also evaluating my RAG system fast, and in such a way that I can rapidly debug/improve the system. And the speed and interpretability criteria are key, in my opinion. In this blog post I show you how to build such system very fast, for rapid prototyping, without having to benchmark, test or dive into complicated frameworks with lots of metrics and settings. My solution is not fancy or complicated, but that is the whole point of it. If your PoC gets validated, you will have plenty of time to select a good RAG testing framework like RAGAS, spend time on elaborating advanced datasets, setup automated testing, etc. (I may dive into these advanced topics in future posts)</p> <h2 id="step-1--dataset-generation">Step 1 — Dataset generation</h2> <p>The dataset generation part is pretty simple. Go over each document chunk of your database and generate a question/answer pair for each chunk with the help of an LLM. Keep track of the chunk used for generation so that we can evaluate the RAG performance later. In other words, output a triple (question, answer, chunk) for each chunk.</p> <p>Here is a piece of code that does just that:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SYSTEM_PROMPT</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are an AI teacher, writing an exam out of course material.
Your task is to generate a (question, answer) pair from a given chunk from the course that is given to you.  
Return a JSON object with two keys:
- </span><span class="sh">'</span><span class="s">question</span><span class="sh">'</span><span class="s">: a question generated from the given chunk
- </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">: the answer to the question
Just return the JSON, without any premamble or comment.

Chunk of the course material:
{chunk}
</span><span class="sh">"""</span>


<span class="k">class</span> <span class="nc">QAPair</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">question</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">question generated from the given chunk</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">answer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">the answer to the question</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>
    <span class="k">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isdir</span><span class="p">(</span>
        <span class="n">args</span><span class="p">.</span><span class="n">output_dir</span>
    <span class="p">),</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Output directory not found: </span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="si">}</span><span class="sh">"</span>
    <span class="k">assert</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="nf">isdir</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">chroma_dir</span><span class="p">),</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Chroma db not found: </span><span class="si">{</span><span class="n">args</span><span class="p">.</span><span class="n">chroma_dir</span><span class="si">}</span><span class="sh">"</span>

    <span class="nf">load_dotenv</span><span class="p">()</span>
    <span class="n">db</span> <span class="o">=</span> <span class="nf">get_db</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">chroma_dir</span><span class="p">)</span>
    <span class="n">llm</span> <span class="o">=</span> <span class="c1"># your llm here
</span>    <span class="n">parser</span> <span class="o">=</span> <span class="nc">JsonOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">QAPair</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
        <span class="n">template</span><span class="o">=</span><span class="n">SYSTEM_PROMPT</span><span class="p">,</span>
        <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">chunk</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()},</span>
    <span class="p">)</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>

    <span class="n">data</span> <span class="o">=</span> <span class="n">db</span><span class="p">.</span><span class="nf">get</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">args</span><span class="p">.</span><span class="n">limit</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">n_chunks</span> <span class="o">=</span> <span class="n">args</span><span class="p">.</span><span class="n">limit</span>
        <span class="n">output_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="sh">"</span><span class="s">qa_dataset_limit=</span><span class="si">{</span><span class="n">n_chunks</span><span class="si">}</span><span class="s">.csv</span><span class="sh">"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">n_chunks</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">output_filename</span> <span class="o">=</span> <span class="sh">"</span><span class="s">qa_dataset.csv</span><span class="sh">"</span>

    <span class="n">dataset</span> <span class="o">=</span> <span class="p">{</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="p">[],</span> <span class="sh">"</span><span class="s">ground_truth_answer</span><span class="sh">"</span><span class="p">:</span> <span class="p">[],</span> <span class="sh">"</span><span class="s">chunk_id</span><span class="sh">"</span><span class="p">:</span> <span class="p">[]}</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="n">n_chunks</span><span class="p">)):</span>
        <span class="n">chunk</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">documents</span><span class="sh">"</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span><span class="sh">"</span><span class="s">chunk</span><span class="sh">"</span><span class="p">:</span> <span class="n">chunk</span><span class="p">})</span>
        <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">ground_truth_answer</span><span class="sh">"</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">])</span>
        <span class="n">dataset</span><span class="p">[</span><span class="sh">"</span><span class="s">chunk_id</span><span class="sh">"</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="sh">"</span><span class="s">ids</span><span class="sh">"</span><span class="p">][</span><span class="n">i</span><span class="p">])</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
    <span class="n">df</span><span class="p">.</span><span class="nf">to_csv</span><span class="p">(</span><span class="nf">str</span><span class="p">(</span><span class="nc">Path</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="n">output_filename</span><span class="p">)),</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>Find the full code example <a href="https://github.com/GTimothee/RAG_experiments/blob/main/test_procedure_for_rag/generate_qa_pairs.py">here</a>.</p> <p>In the following of the post I only use a test set, but of course you can split it into a validation set and a test set, ensuring that the proportion of each source document is approximately the same in each set.</p> <h2 id="step-2--procedures-pseudo-code">Step 2 — Procedure’s pseudo code</h2> <p>Without further delay, let’s have a look at the full test procedure:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">each</span> <span class="n">document</span>
  <span class="k">for</span> <span class="n">each</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">document</span>
    <span class="c1"># 1- retrieve the answer from your system
</span>    <span class="n">run</span> <span class="n">your</span> <span class="n">RAG</span> <span class="n">system</span> <span class="n">on</span> <span class="n">the</span> <span class="n">question</span>
    <span class="n">gather</span> <span class="n">the</span> <span class="n">answer</span> <span class="ow">and</span> <span class="n">the</span> <span class="nb">list</span> <span class="n">of</span> <span class="n">chunks</span> <span class="n">retrieved</span> <span class="ow">and</span> <span class="n">used</span> <span class="k">as</span> <span class="n">context</span>
    
    <span class="c1"># 2- evaluate
</span>    <span class="n">compute</span> <span class="n">your</span> <span class="n">performance</span> <span class="n">metrics</span> <span class="n">of</span> <span class="n">the</span> <span class="n">whole</span> <span class="n">system</span> <span class="n">by</span> <span class="n">comparing</span> <span class="n">the</span> <span class="n">answer</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">ground_truth</span>
    <span class="n">compute</span> <span class="n">your</span> <span class="n">rag</span> <span class="n">performance</span> <span class="n">by</span> <span class="n">saving</span> <span class="n">the</span> <span class="n">rank</span> <span class="n">of</span> <span class="n">the</span> <span class="n">target</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">contexts</span> <span class="nb">list</span> <span class="n">that</span> <span class="n">has</span> <span class="n">been</span> <span class="n">retrieved</span> <span class="n">by</span> <span class="n">your</span> <span class="n">system</span><span class="p">.</span> <span class="n">If</span> <span class="n">the</span> <span class="n">target</span> <span class="n">context</span> <span class="ow">is</span> <span class="ow">not</span> <span class="n">there</span><span class="p">,</span> <span class="n">the</span> <span class="n">rank</span> <span class="ow">is</span> <span class="nb">set</span> <span class="n">to</span> <span class="bp">None</span> <span class="ow">or</span> <span class="n">something</span> <span class="n">equivalent</span><span class="p">.</span>
    <span class="n">Save</span> <span class="n">everything</span> <span class="k">as</span> <span class="n">a</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">csv</span> <span class="nb">file</span>
</code></pre></div></div> <p>That’s it. You get a csv file as output with all the data you need, you can now compute statistics like the completeness, conciseness and ranking distributions, the %match, %misses. A nice to have is to compute statistics per document (add a column to the csv file with the index or the name of the document).</p> <p>Here is an interpretation of the first part of the peudocode, to generate answers from the dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rag_chain</span><span class="p">,</span> <span class="n">retriever</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="nf">get_rag_chain_eval</span><span class="p">(</span><span class="n">chroma_db_dirpath</span><span class="o">=</span><span class="n">path_to_your_db</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">dataset_filepath</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">answers</span><span class="sh">'</span><span class="p">:</span> <span class="p">[],</span> <span class="sh">'</span><span class="s">ranks</span><span class="sh">'</span><span class="p">:</span> <span class="p">[]</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">itertuples</span><span class="p">(),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">'</span><span class="s">Generating answers...</span><span class="sh">'</span><span class="p">):</span>
    <span class="n">documents</span> <span class="o">=</span> <span class="n">retriever</span><span class="p">.</span><span class="nf">invoke</span><span class="p">(</span><span class="n">row</span><span class="p">.</span><span class="n">question</span><span class="p">)</span>

    <span class="c1"># generate answer
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">rag_chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">.</span><span class="n">question</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">context</span><span class="sh">"</span><span class="p">:</span> <span class="sh">'</span><span class="se">\n</span><span class="sh">'</span><span class="p">.</span><span class="nf">join</span><span class="p">([</span><span class="n">doc</span><span class="p">.</span><span class="n">page_content</span> <span class="k">for</span> <span class="n">doc</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">])</span>
    <span class="p">})</span>
    <span class="n">outputs</span><span class="p">[</span><span class="sh">'</span><span class="s">answers</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

    <span class="c1"># compute rank of the target documents in the list of retrieved documents
</span>    <span class="n">target_chunk</span> <span class="o">=</span> <span class="n">db</span><span class="p">.</span><span class="nf">get</span><span class="p">(</span><span class="n">row</span><span class="p">.</span><span class="n">chunk_id</span><span class="p">)[</span><span class="sh">'</span><span class="s">documents</span><span class="sh">'</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">rank</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="n">documents</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">chunk</span><span class="p">.</span><span class="n">page_content</span> <span class="o">==</span> <span class="n">target_chunk</span><span class="p">:</span>
            <span class="n">rank</span> <span class="o">=</span> <span class="n">i</span>
    <span class="n">outputs</span><span class="p">[</span><span class="sh">'</span><span class="s">ranks</span><span class="sh">'</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">rank</span><span class="p">)</span>

<span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">outputs</span><span class="p">).</span><span class="nf">to_csv</span><span class="p">(</span>
    <span class="nf">str</span><span class="p">(</span><span class="nc">Path</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">output_dir</span><span class="p">,</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nc">Path</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">dataset_filepath</span><span class="p">).</span><span class="n">stem</span><span class="si">}</span><span class="sh">"</span> <span class="o">+</span> <span class="sh">"</span><span class="s">_answers.csv</span><span class="sh">"</span><span class="p">)),</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>You can find the full code <a href="https://github.com/GTimothee/RAG_experiments/blob/main/test_procedure_for_rag/generate_answers.py">here</a></p> <p>Below is an interpretation of the second part of the pseudocode, to evaluate the answers. I use two metrics, completeness and conciseness to evaluate the RAG answers.</p> <p>Completeness evaluates whether or not the answer answers the question, while conciseness answers the question “how much of the answer is actually relevant”. If the completeness is low, then the system had trouble retrieving the relevant documents. If the conciseness is low, and the completeness is high, you are retrieving too much documents. So try to focus on improving the rank of the target document in the set of retrieved documents so that you can reduce the number of documents retrieved and reduce the noise. You could also add a reranker, which is probably a good idea in any RAG system.</p> <p>Additional comments about the evaluation prompt:</p> <ul> <li>I tried adding some other keys like ‘comments’ or ‘reasons’ to leverage the idea of chain of thoughts, but it did not provide any useful information</li> <li>I use floats here, but it may be that using integers from 1 to 10 instead would be more efficient or precise.</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">SYSTEM_PROMPT</span> <span class="o">=</span> <span class="sh">"""</span><span class="s">You are a top-tier grading software belonging to a school.
Your task is to give a grade to evaluate the answer goodness to a given question, given the ground truth answer.

You will be given a piece of data containing: 
- a </span><span class="sh">'</span><span class="s">question</span><span class="sh">'</span><span class="s">
- an </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">: the answer to the question from the student
- a </span><span class="sh">'</span><span class="s">ground truth answer</span><span class="sh">'</span><span class="s">: the expected answer to the question

Provide your answer as a JSON with two keys: 
- </span><span class="sh">'</span><span class="s">completeness</span><span class="sh">'</span><span class="s">: A float between 0 and 1. The percentage of the ground truth answer that is present in the student</span><span class="sh">'</span><span class="s">s answer. A score of 1 means that all the information in the </span><span class="sh">'</span><span class="s">ground truth answer</span><span class="sh">'</span><span class="s"> can be found in the </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">. No matter if the answer contains more information than expected. A score of 0 means that no information present in the </span><span class="sh">'</span><span class="s">ground truth answer</span><span class="sh">'</span><span class="s"> can be found in the </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">.
- </span><span class="sh">'</span><span class="s">conciseness</span><span class="sh">'</span><span class="s">: A float between 0 and 1. The percentage of the answer that is part of the ground truth. Conciseness measures how much of the answer is really useful.

Here is the data to evaluate: 
- </span><span class="sh">'</span><span class="s">question</span><span class="sh">'</span><span class="s">: {question}
- </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">: {answer}
- </span><span class="sh">'</span><span class="s">ground truth answer</span><span class="sh">'</span><span class="s">: {ground_truth_answer}

Provide your answer as a JSON, with no additional text.
</span><span class="sh">"""</span>


<span class="k">class</span> <span class="nc">Evaluation</span><span class="p">(</span><span class="n">BaseModel</span><span class="p">):</span>
    <span class="n">completeness</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A float between 0 and 1. The percentage of the ground truth answer that is present in the student</span><span class="sh">'</span><span class="s">s answer. A score of 1 means that all the information in the </span><span class="sh">'</span><span class="s">ground truth answer</span><span class="sh">'</span><span class="s"> can be found in the </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">. No matter if the answer contains more information than expected. A score of 0 means that no information present in the </span><span class="sh">'</span><span class="s">ground truth answer</span><span class="sh">'</span><span class="s"> can be found in the </span><span class="sh">'</span><span class="s">answer</span><span class="sh">'</span><span class="s">.</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">conciseness</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="nc">Field</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="sh">"</span><span class="s">A float between 0 and 1. The percentage of the answer that is part of the ground truth. Conciseness measures how much of the answer is really useful.</span><span class="sh">"</span><span class="p">)</span>


<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="sh">"</span><span class="s">__main__</span><span class="sh">"</span><span class="p">:</span>

    <span class="nf">load_dotenv</span><span class="p">()</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span>
        <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">dataset_filepath</span><span class="p">),</span>
        <span class="n">pd</span><span class="p">.</span><span class="nf">read_csv</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">answers_filepath</span><span class="p">)</span>
    <span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">llm</span> <span class="o">=</span> <span class="nc">OpenAI</span><span class="p">(</span>
        <span class="n">openai_api_base</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">OPENAI_BASE_URL</span><span class="sh">"</span><span class="p">),</span>
        <span class="n">openai_api_key</span><span class="o">=</span><span class="n">os</span><span class="p">.</span><span class="nf">getenv</span><span class="p">(</span><span class="sh">"</span><span class="s">OPENAI_API_KEY</span><span class="sh">"</span><span class="p">),</span>
        <span class="n">model_name</span><span class="o">=</span><span class="sh">"</span><span class="s">Llama-3-70B-Instruct</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">temperature</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="nc">JsonOutputParser</span><span class="p">(</span><span class="n">pydantic_object</span><span class="o">=</span><span class="n">Evaluation</span><span class="p">)</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="nc">PromptTemplate</span><span class="p">(</span>
        <span class="n">template</span><span class="o">=</span><span class="n">SYSTEM_PROMPT</span><span class="p">,</span>
        <span class="n">input_variables</span><span class="o">=</span><span class="p">[</span><span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">ground_truth_answer</span><span class="sh">"</span><span class="p">],</span>
        <span class="n">partial_variables</span><span class="o">=</span><span class="p">{</span><span class="sh">"</span><span class="s">format_instructions</span><span class="sh">"</span><span class="p">:</span> <span class="n">parser</span><span class="p">.</span><span class="nf">get_format_instructions</span><span class="p">()},</span>
    <span class="p">)</span>
    <span class="n">chain</span> <span class="o">=</span> <span class="n">prompt</span> <span class="o">|</span> <span class="n">llm</span> <span class="o">|</span> <span class="n">parser</span>

    <span class="n">conciseness</span><span class="p">,</span> <span class="n">completeness</span> <span class="o">=</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span>
    <span class="n">ranks</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="nf">tqdm</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="nf">itertuples</span><span class="p">(),</span> <span class="n">total</span><span class="o">=</span><span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">),</span> <span class="n">desc</span><span class="o">=</span><span class="sh">'</span><span class="s">Evaluating answers...</span><span class="sh">'</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">chain</span><span class="p">.</span><span class="nf">invoke</span><span class="p">({</span>
            <span class="sh">"</span><span class="s">question</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">.</span><span class="n">question</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">.</span><span class="n">answers</span><span class="p">,</span>
            <span class="sh">"</span><span class="s">ground_truth_answer</span><span class="sh">"</span><span class="p">:</span> <span class="n">row</span><span class="p">.</span><span class="n">ground_truth_answer</span>
        <span class="p">})</span>
        <span class="n">completeness</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="sh">'</span><span class="s">completeness</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">conciseness</span> <span class="o">+=</span> <span class="n">output</span><span class="p">[</span><span class="sh">'</span><span class="s">conciseness</span><span class="sh">'</span><span class="p">]</span>
        <span class="n">ranks</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">row</span><span class="p">.</span><span class="n">ranks</span><span class="p">)</span>
    
    <span class="n">mean_conciseness</span> <span class="o">=</span> <span class="n">conciseness</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
    <span class="n">mean_completeness</span> <span class="o">=</span> <span class="n">completeness</span> <span class="o">/</span> <span class="nf">len</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>

    <span class="nf">print</span><span class="p">({</span>
        <span class="sh">"</span><span class="s">mean_completeness</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">mean_completeness</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s"> %</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">mean_conciseness</span><span class="sh">"</span><span class="p">:</span> <span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">mean_conciseness</span><span class="o">*</span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s"> %</span><span class="sh">"</span>
    <span class="p">})</span>

    <span class="nf">print</span><span class="p">(</span><span class="n">pd</span><span class="p">.</span><span class="nc">Series</span><span class="p">(</span><span class="n">ranks</span><span class="p">).</span><span class="nf">value_counts</span><span class="p">())</span>
</code></pre></div></div> <p>Again, the full code is <a href="https://github.com/GTimothee/RAG_experiments/blob/main/test_procedure_for_rag/evaluate.py">here</a></p> <h2 id="disclaimer-it-is-for-qa-evaluation">Disclaimer: It is for Q/A evaluation</h2> <p>By Q/A evaluation I mean that each question of the test set is associated to (and can be answered with) one document chunk. Consequently, this evaluation procedure is for Q/A RAG only; Indeed, if you are looking for an answer for which you must gather data from multiple chunks, this procedure would not evaluate that. Still, I believe it is a good starting point when evaluating your RAG, as if you cannot reliably find one document chunk, how could you find multiple target document chunks? You could probably start with this procedure and then add another procedure for more complex use cases.</p> <h2 id="sources">Sources</h2> <p>To write the examples of this blog post, I relied on these two pages:</p> <ul> <li>https://huggingface.co/learn/cookbook/en/advanced_rag</li> <li>https://python.langchain.com/docs/tutorials/rag/#retrieval-and-generation-generate</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>Now you have a pretty good idea of how your RAG app performs. Now every time you want to add a document to the knowledge base, add it to the dataset and run the test. You will know how much the addition of the new chunks in the database interferes with the existing documents, and what is the performance of your RAG system on your new document.</p> <p>Medium link: https://medium.com/@timothee.guedon/simple-test-procedure-and-metrics-for-your-rag-in-5-minutes-a86b329a5f7a</p>]]></content><author><name></name></author><category term="rag"/><category term="llm"/><category term="rag"/><category term="evaluation"/><category term="procedure"/><category term="metrics"/><summary type="html"><![CDATA[Simple test procedure and metrics for your RAG in 5 minutes]]></summary></entry></feed>