<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://gtimothee.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://gtimothee.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-04T05:45:13+00:00</updated><id>https://gtimothee.github.io/feed.xml</id><title type="html">blank</title><subtitle></subtitle><entry><title type="html">Simple test procedure for RAG in 5 minutes</title><link href="https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag/" rel="alternate" type="text/html" title="Simple test procedure for RAG in 5 minutes"/><published>2024-10-04T00:00:00+00:00</published><updated>2024-10-04T00:00:00+00:00</updated><id>https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag</id><content type="html" xml:base="https://gtimothee.github.io/blog/2024/simple-test-procedure-for-rag/"><![CDATA[<h1 id="simple-test-procedure-for-your-qa-rag-without-external-libraries-in-5-minutes">Simple test procedure for your Q/A RAG without external libraries in 5 minutes</h1> <p>Q/A because of course if you are looking for an answer for which you must gather data from multiple chunks, this procedure would not evaluate that. Still, this procedure could probably be adapted to that use case (answering based on multiple chunks).</p> <p>for each document for each chunk in the document generate a (question, answer) pair rename answer as ground truth save the (question, ground truth, context) triple run your system on the question gather the answer and the documents used as context compute performance metrics of the whole system by comparing answer and ground truth compute rag performance by saving the rank of the target context in the contexts list that have been retrieved by the system. If the target context is not there, rank is set to None Save everything</p> <p>You get a csv file with all the data, you can now compute statistics like completeness, conciseness, ranking distribution, %match, %misses. A nice to have is to compute statistics per document (add a column to the csv file with the index or the name of the document.</p> <p>Now you have a pretty good idea of how your rag app performs. Now every time you want to add a document to the knowledge base, add it to the test and run the test. You will know how much the addition of the new chunks in the database interferes with the existing documents, and what is the performance of your rag system on your new document.</p> <p>of course you can also define a validation set and a test set</p>]]></content><author><name></name></author><category term="rag"/><category term="llm"/><category term="rag"/><category term="evaluation"/><category term="procedure"/><summary type="html"><![CDATA[A simple test procedure for RAG without use of external libraries]]></summary></entry></feed>